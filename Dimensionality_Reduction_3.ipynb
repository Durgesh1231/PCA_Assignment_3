{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example.\n",
        "import numpy as np\n",
        "\n",
        "# Define a matrix A\n",
        "A = np.array([[4, 1], [2, 3]])\n",
        "\n",
        "# Compute the eigenvalues and eigenvectors\n",
        "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
        "\n",
        "print(\"Q1: Eigenvalues: \", eigenvalues)\n",
        "print(\"Q1: Eigenvectors: \\n\", eigenvectors)\n",
        "\n",
        "# The relationship between eigenvalues and eigenvectors is:\n",
        "# A * v = 位 * v\n",
        "# Where:\n",
        "# A = Matrix, v = Eigenvector, 位 = Eigenvalue\n",
        "\n",
        "\n",
        "# Q2. What is eigen decomposition and what is its significance in linear algebra?\n",
        "# Eigen decomposition decomposes a matrix A into eigenvectors and eigenvalues\n",
        "# A = V * Lambda * V^-1\n",
        "# Where V is the matrix of eigenvectors and Lambda is the diagonal matrix of eigenvalues\n",
        "\n",
        "# Reconstruct the matrix using the eigen decomposition\n",
        "A_reconstructed = eigenvectors @ np.diag(eigenvalues) @ np.linalg.inv(eigenvectors)\n",
        "print(\"Q2: Reconstructed matrix from eigen decomposition: \\n\", A_reconstructed)\n",
        "\n",
        "\n",
        "# Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer.\n",
        "# A matrix is diagonalizable if it has n linearly independent eigenvectors\n",
        "# The matrix must be square, and it should have a full set of eigenvectors\n",
        "\n",
        "# Example:\n",
        "A = np.array([[4, 1], [2, 3]])\n",
        "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
        "\n",
        "# If eigenvectors are linearly independent (i.e., matrix V is invertible), then it is diagonalizable\n",
        "if np.linalg.matrix_rank(eigenvectors) == A.shape[0]:\n",
        "    print(\"Q3: Matrix A is diagonalizable.\")\n",
        "else:\n",
        "    print(\"Q3: Matrix A is not diagonalizable.\")\n",
        "\n",
        "\n",
        "# Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example.\n",
        "# Spectral theorem states that a symmetric matrix can be diagonalized by an orthogonal matrix\n",
        "# Symmetric matrix example\n",
        "A_symmetric = np.array([[4, 1], [1, 3]])\n",
        "\n",
        "# Compute eigenvalues and eigenvectors\n",
        "eigenvalues, eigenvectors = np.linalg.eig(A_symmetric)\n",
        "\n",
        "# Since the matrix is symmetric, eigenvectors are orthogonal, and it is diagonalizable\n",
        "print(\"Q4: Eigenvalues: \", eigenvalues)\n",
        "print(\"Q4: Eigenvectors (orthogonal matrix): \\n\", eigenvectors)\n",
        "\n",
        "\n",
        "# Q5. How do you find the eigenvalues of a matrix and what do they represent?\n",
        "# Finding eigenvalues using numpy\n",
        "A = np.array([[4, 1], [2, 3]])\n",
        "\n",
        "# Compute eigenvalues\n",
        "eigenvalues, _ = np.linalg.eig(A)\n",
        "print(\"Q5: Eigenvalues: \", eigenvalues)\n",
        "\n",
        "# Eigenvalues represent the scaling factor in the direction of the eigenvectors\n",
        "\n",
        "\n",
        "# Q6. What are eigenvectors and how are they related to eigenvalues?\n",
        "# Eigenvectors are the directions in which a matrix transformation acts by scaling\n",
        "# The corresponding eigenvalue represents how much the matrix stretches or shrinks the eigenvector\n",
        "A = np.array([[4, 1], [2, 3]])\n",
        "\n",
        "# Compute eigenvalues and eigenvectors\n",
        "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
        "\n",
        "print(\"Q6: Eigenvalues: \", eigenvalues)\n",
        "print(\"Q6: Eigenvectors: \\n\", eigenvectors)\n",
        "\n",
        "# The relationship is A * v = 位 * v\n",
        "# Where A is the matrix, v is the eigenvector, and 位 is the eigenvalue\n",
        "\n",
        "\n",
        "# Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n",
        "# Geometrically, eigenvectors define directions in which the transformation only scales,\n",
        "# not rotates. Eigenvalues represent how much the vector is stretched or shrunk along those directions.\n",
        "# Example of scaling transformation on a vector\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "v = np.array([1, 2])  # Eigenvector\n",
        "A = np.array([[4, 1], [2, 3]])  # Matrix\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_v = A.dot(v)\n",
        "\n",
        "plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color=\"blue\", label=\"Eigenvector\")\n",
        "plt.quiver(0, 0, transformed_v[0], transformed_v[1], angles='xy', scale_units='xy', scale=1, color=\"red\", label=\"Transformed Vector\")\n",
        "\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(-5, 5)\n",
        "plt.grid(True)\n",
        "plt.axhline(0, color='black',linewidth=0.5)\n",
        "plt.axvline(0, color='black',linewidth=0.5)\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Q8. What are some real-world applications of eigen decomposition?\n",
        "# 1. Principal Component Analysis (PCA) for dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# Apply PCA to reduce dimensions\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(\"Q8: Transformed data after PCA:\\n\", X_pca)\n",
        "\n",
        "# 2. Image compression using Singular Value Decomposition (SVD) - a form of eigen decomposition\n",
        "from numpy.linalg import svd\n",
        "import cv2\n",
        "\n",
        "# Example of image compression using SVD\n",
        "img = cv2.imread('image_path.jpg', 0)  # Read image in grayscale\n",
        "U, S, Vt = svd(img, full_matrices=False)\n",
        "\n",
        "# Reconstruct the image using only the first 50 singular values\n",
        "reconstructed_img = np.dot(U[:, :50], np.dot(np.diag(S[:50]), Vt[:50, :]))\n",
        "plt.imshow(reconstructed_img, cmap='gray')\n",
        "plt.title(\"Compressed Image\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?\n",
        "# Yes, a matrix can have multiple eigenvectors corresponding to the same eigenvalue. If an eigenvalue has multiplicity greater than one, the eigenspace corresponding to that eigenvalue will have more than one eigenvector.\n",
        "\n",
        "\n",
        "# Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.\n",
        "# 1. PCA for dimensionality reduction\n",
        "# Already demonstrated in Q8.\n",
        "\n",
        "# 2. Image compression (SVD) - Singular Value Decomposition, a form of eigen-decomposition\n",
        "# Example code provided in Q8 for image compression.\n",
        "\n",
        "# 3. Spectral clustering in machine learning - Eigen decomposition of the Laplacian matrix\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from scipy.sparse.linalg import eigsh\n",
        "\n",
        "# Generate sample data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Compute the similarity matrix (Laplacian)\n",
        "A = kneighbors_graph(X, n_neighbors=10, include_self=False)\n",
        "L = np.diag(A.sum(axis=1)) - A  # Laplacian matrix\n",
        "\n",
        "# Perform eigen decomposition\n",
        "eigenvalues, eigenvectors = eigsh(L, k=2, which='SM')  # Find the smallest 2 eigenvalues and eigenvectors\n",
        "\n",
        "# Apply KMeans on eigenvectors\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(eigenvectors)\n",
        "\n",
        "print(\"Q10: Cluster labels: \", kmeans.labels_)\n"
      ]
    }
  ]
}